{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Face Mesh module\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.3)\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "def calculate_emotion(landmarks, image_width, image_height):\n",
    "    # Core facial landmarks for normalization\n",
    "    left_eye_inner = landmarks[133]\n",
    "    right_eye_inner = landmarks[362]\n",
    "    eye_distance = np.linalg.norm(np.array(left_eye_inner) - np.array(right_eye_inner))\n",
    "\n",
    "    # Eye Openness\n",
    "    left_eye_top = landmarks[159]  # Check MediaPipe index\n",
    "    left_eye_bottom = landmarks[145]  # Check MediaPipe index\n",
    "    right_eye_top = landmarks[386]  # Check MediaPipe index\n",
    "    right_eye_bottom = landmarks[374]  # Check MediaPipe index\n",
    "    left_eye_openness = np.linalg.norm(np.array(left_eye_top) - np.array(left_eye_bottom)) / eye_distance\n",
    "    right_eye_openness = np.linalg.norm(np.array(right_eye_top) - np.array(right_eye_bottom)) / eye_distance\n",
    "    average_eye_openness = (left_eye_openness + right_eye_openness) / 2\n",
    "\n",
    "    # Nose Wrinkle\n",
    "    nose_top = landmarks[6]  # Check MediaPipe index\n",
    "    nose_bottom = landmarks[4]  # Check MediaPipe index\n",
    "    nose_wrinkle = np.linalg.norm(np.array(nose_top) - np.array(nose_bottom)) / eye_distance\n",
    "\n",
    "    # Lip Corner Puller\n",
    "    lip_left_corner = landmarks[61]  # Check MediaPipe index\n",
    "    lip_right_corner = landmarks[291]  # Check MediaPipe index\n",
    "    lip_corner_distance = np.linalg.norm(np.array(lip_left_corner) - np.array(lip_right_corner)) / eye_distance\n",
    "\n",
    "    # Chin Raiser\n",
    "    chin_bottom = landmarks[152]  # Check MediaPipe index\n",
    "    lip_bottom = landmarks[17]  # Check MediaPipe index\n",
    "    chin_raiser = np.linalg.norm(np.array(chin_bottom) - np.array(lip_bottom)) / eye_distance\n",
    "\n",
    "    # Lip Pressor\n",
    "    lip_upper = landmarks[13]  # Check MediaPipe index\n",
    "    lip_lower = landmarks[14]  # Check MediaPipe index\n",
    "    lip_pressure = np.linalg.norm(np.array(lip_upper) - np.array(lip_lower)) / eye_distance\n",
    "    print(f'Eye Openness: {average_eye_openness:.2f}, Nose Wrinkle: {nose_wrinkle:.2f}, Lip Corner Distance: {lip_corner_distance:.2f}, Chin Raiser: {chin_raiser:.2f}, Lip Pressure: {lip_pressure:.2f}')\n",
    "    # Emotion Detection Logic with additional feature\n",
    "    if lip_corner_distance > 1.5 and average_eye_openness > 0.2 and lip_pressure <  0.4:\n",
    "        return \"Smiling\"\n",
    "    elif nose_wrinkle < 0.87 and lip_pressure > 0.1 and lip_corner_distance < 1.55  and average_eye_openness< 0.26:\n",
    "        return \"Disgusted\"\n",
    "    elif lip_pressure <= 0.02 and average_eye_openness < 0.23:\n",
    "        return \"Sad\"\n",
    "    elif average_eye_openness > 0.25 and lip_pressure > 0.2 :\n",
    "        return \"Surprised\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Continue with your webcam capture and processing logic as before\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    original_image = image.copy()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Convert the BGR image to RGB and upscale\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb = cv2.resize(image_rgb, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Process it with MediaPipe Face Mesh\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Draw the face mesh annotations on the image.\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=1))\n",
    "\n",
    "            # Convert landmarks to an array for easier manipulation\n",
    "            landmark_array = np.array([(lm.x * image.shape[1], lm.y * image.shape[0]) for lm in face_landmarks.landmark])\n",
    "\n",
    "            # Detect emotion\n",
    "            emotion = calculate_emotion(landmark_array, image.shape[1], image.shape[0])\n",
    "\n",
    "            # Display detected emotion\n",
    "            cv2.putText(image, f'Emotion: {emotion}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the image\n",
    "    # Concatenate original and processed image horizontally\n",
    "    comparison_frame = np.hstack((original_image, image))\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Original vs Processed Emotion Detection', comparison_frame)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
